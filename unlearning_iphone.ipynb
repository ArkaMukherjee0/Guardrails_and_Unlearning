{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the iPhone 16 dataset from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017f701df8ad45d09fbc66daff1f16bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/415 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021ec312ce6e4466ac4301b3781fb02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/15.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae293e5094f47baa59c345659c895ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d471e41c784a6ca2b4c94bda3e8d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76069a50fac84c1cbf5409778354f703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# load and prepare dataset\n",
    "ds = load_dataset(\"ArkaMukherjee/iphone16-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaVA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"LLaVA\"):\n",
    "    !git clone https://github.com/haotian-liu/LLaVA\n",
    "else:\n",
    "    print(\"LLaVA directory already exists. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "import transformers\n",
    "from transformers import AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "import torch.nn as nn\n",
    "\n",
    "#cuda_idx = 2 # edit device index that you want to track\n",
    "#device = f'cuda:{cuda_idx}'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.6-mistral-7b\"\n",
    "#model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "#model_path = \"liuhaotian/llava-v1.6-vicuna-7b\"\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "print(model_name)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    cache_dir='',\n",
    "    use_flash_attn=True,\n",
    "    device=device,\n",
    "    #load_8bit = #NOT SUPPORTED\n",
    "    #load_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "import re\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common function to create prompts\n",
    "def create_prompt (query, model, model_name=model_name, caption=None):\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in query:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            query = re.sub(IMAGE_PLACEHOLDER, image_token_se, query)\n",
    "        else:\n",
    "            query = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, query)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            query = image_token_se + \"\\n\" + query\n",
    "        else:\n",
    "            query = DEFAULT_IMAGE_TOKEN + \"\\n\" + query\n",
    "            \n",
    "    conv_mode = infer_conv_mode(model_name)\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], query)\n",
    "    if caption is not None:\n",
    "        conv.append_message(conv.roles[1], caption)\n",
    "    else:\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "# Common function to infer conversation mode\n",
    "def infer_conv_mode(model_name):\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        return \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        return \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        return \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        return \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        return \"mpt\"\n",
    "    else:\n",
    "        return \"llava_v0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import PIL.Image\n",
    "\n",
    "def load_image(image_input):\n",
    "    # Check if the input is a string (path or URL)\n",
    "    if isinstance(image_input, str):\n",
    "        if image_input.startswith(\"http\") or image_input.startswith(\"https\"):\n",
    "            response = requests.get(image_input)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(image_input).convert(\"RGB\")\n",
    "    elif isinstance(image_input, PIL.Image.Image):\n",
    "        # Input is already an Image object, return as is\n",
    "        image = image_input\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image input type\")\n",
    "    return image\n",
    "\n",
    "# Common function to process images\n",
    "def process_and_prepare_images(image_files, image_processor, model, device):\n",
    "    images = [load_image(image_file) for image_file in image_files]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(\n",
    "        device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    image_sizes = [image.size for image in images]\n",
    "    return images_tensor, image_sizes\n",
    "\n",
    "def eval_model(tokenizer, model, image_processor, context_len, image_file, query, model_name=model_name, sep=\",\", temperature=1.0, num_beams=1, max_new_tokens=512):\n",
    "    # Model\n",
    "    disable_torch_init()\n",
    "    \n",
    "    # Create prompt using the common function\n",
    "    prompt = create_prompt(query, model, model_name)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Process images using the common function\n",
    "    if isinstance(image_file, list):\n",
    "        images_tensor, image_sizes = process_and_prepare_images(image_file, image_processor, model, model.device)\n",
    "    elif isinstance(image_file, str):\n",
    "        images_tensor, image_sizes = process_and_prepare_images([image_file], image_processor, model, model.device)\n",
    "    else:\n",
    "        # If image_file is neither a list nor a string, it's likely an Image object or similar; wrap it in a list\n",
    "        images = [image_file]\n",
    "        images_tensor, image_sizes = process_and_prepare_images(images, image_processor, model, model.device)\n",
    "\n",
    "    images_tensor.to(model.device)\n",
    "    image_sizes.to(device) if isinstance(image_sizes, torch.Tensor) else image_sizes\n",
    "    \n",
    "    # Tokenixe the prompt using the custom tokenizer_image_token function\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .to(model.device)\n",
    "    )\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=temperature != 1.0,\n",
    "            temperature=temperature,\n",
    "            # top_p=top_p,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=False)[0].strip()\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference on original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Dog image URL\n",
    "image_url = 'https://farm7.staticflickr.com/6119/6315804553_050a2d1f4e_z.jpg'\n",
    "\n",
    "# Download the image and open it with PIL\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Display the image using matplotplib\n",
    "plt.imshow(image)\n",
    "plt.axis('off') # Turn off axis numbers and ticks\n",
    "plt.show()\n",
    "\n",
    "# Pass the processed image to eval_model\n",
    "eval_model(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    image_processor,\n",
    "    context_len,\n",
    "    image, # using the processed image\n",
    "    \"What do you see in this picure?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_create_labels(example_batch, image_processor, tokenizer, model, device, model_name=model_name, ignore_index=-100, image_token_index=IMAGE_TOKEN_INDEX):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = example_batch['image']\n",
    "    \n",
    "    images_tensor, image_sizes = process_and_prepare_images(image_files, image_processor, model, device)\n",
    "\n",
    "    query = \"What do you see in this picture?\"\n",
    "    \n",
    "    # Tokenize the conversation without the captions to determine which tokens to ignore\n",
    "    tokenized_conversations_without_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, None), tokenizer, image_token_index, return_tensors=\"pt\")\n",
    "        for _ in example_batch['captions']\n",
    "    ]\n",
    "    \n",
    "    # Tokenize the full conversations with the captions\n",
    "    tokenized_conversations_with_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, caption), tokenizer, image_token_index, return_tensors=\"pt\")\n",
    "        for caption in example_batch['captions']\n",
    "    ]\n",
    "    \n",
    "    # Pad the tokenized conversations to the same length\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversations_with_caption], batch_first=True, padding_value=pad_token_id).to(device)\n",
    "    \n",
    "    # Create attention_mask (1 for real tokens and 0 for padding tokens)\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "    \n",
    "    # Create the labels tensor which is a copy of input_ids but with ignore_index for non-caption tokens\n",
    "    labels = torch.full_like(input_ids, fill_value=ignore_index)\n",
    "    for i, tcwc in enumerate(tokenized_conversations_without_caption):\n",
    "        # Set ignore_index for the tokens corresponding to the conversation without the caption\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
    "\n",
    "    #print(\"Labels shape: \", labels.shape)\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"images\": images_tensor,\n",
    "        \"image_sizes\": image_sizes,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "    \n",
    "# Make sure to define the function outside of the lambda to ensure it's serializable\n",
    "def transform_batch(batch):\n",
    "    return tokenize_and_create_labels(batch, image_processor, tokenizer, model, device, model_name)\n",
    "\n",
    "train_ds = ds[\"train\"] #train_dataset\n",
    "eval_ds = ds[\"test\"] #test_dataset\n",
    "\n",
    "# Apply the transformation function to the dataset\n",
    "train_ds.set_transform(transform_batch)\n",
    "eval_ds.set_transform(transform_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference on entire eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temporarily disable the transformation to access the original data\n",
    "eval_ds.reset_format()\n",
    "\n",
    "# Iterate over each example in the evaluation dataset\n",
    "for i in range(len(eval_ds)):\n",
    "    # Access the original image and caption for the current row\n",
    "    image = eval_ds[i]['image']\n",
    "    caption = eval_ds[i]['caption']\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "\n",
    "    eval_model(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        image_processor,\n",
    "        context_len,\n",
    "        image,\n",
    "        \"What do you see in this picture?\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
    "\n",
    "# Re-enable the transformation if needed\n",
    "eval_ds.set_transform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        # \"fc1\", \"fc2\", # for llama,\n",
    "        \"mm_projector\" #for mistral, train instead \"mm_projector\"\n",
    "        \"up_proj\", \"down_proj\", \"gate_proj\" #optionally train more linarly\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.to(device)\n",
    "#model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_ds is your training dataset prepared as a PyTorch Dataset object\n",
    "batch_size = 4  # Specify the batch size you want to use\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Assuming train_loader is your DataLoader instance for the training dataset\n",
    "for batch in train_loader:\n",
    "    batch_size = len(batch)\n",
    "    print(batch.keys())  # Print the dictionary keys to see what data is included in a batch\n",
    "    \n",
    "    # If 'images' is a key, this indicates that images are being loaded\n",
    "    if 'images' in batch:\n",
    "        print(\"Images are included in the DataLoader.\")\n",
    "        print(f\"Batch 'images' shape: {batch['images'].shape}\")  # Print the shape of the images tensor\n",
    "        \n",
    "    # Similarly, check for other expected keys, like 'input_ids' and 'attention_mask'\n",
    "    if 'input_ids' in batch and 'attention_mask' in batch:\n",
    "        # Print the first row of input_ids to check for out-of-range token IDs\n",
    "        input_ids_first_row = batch['input_ids'][1]\n",
    "        print(f\"First row of 'input_ids': \\n{input_ids_first_row.tolist()}\")\n",
    "\n",
    "        # # Check if any token IDs are out of range\n",
    "        # vocab_size = tokenizer.vocab_size\n",
    "        # out_of_range_tokens = [token_id for token_id in input_ids_first_row if token_id >= vocab_size]\n",
    "        # if out_of_range_tokens:\n",
    "        #     print(f\"Out-of-range token IDs: {out_of_range_tokens}\")\n",
    "\n",
    "        # # Decode the first row of input_ids to text, if all token IDs are in range\n",
    "        # if not out_of_range_tokens:\n",
    "        #     decoded_inputs = tokenizer.decode(input_ids_first_row, skip_special_tokens=False)\n",
    "        #     print(f\"Decoded input tokens: {decoded_inputs}\")\n",
    "        # else:\n",
    "        #     print(\"Cannot decode input_ids due to out-of-range token IDs.\")\n",
    "            \n",
    "        print(\"Text inputs are included in the DataLoader.\")\n",
    "        print(f\"Batch 'input_ids' shape: {batch['input_ids'].shape}\")\n",
    "        print(f\"Batch 'attention_mask' shape: {batch['attention_mask'].shape}\")\n",
    "        \n",
    "        # # Decode the first row of input_ids to text\n",
    "        # decoded_inputs = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=False)\n",
    "        # print(f\"Decoded input tokens: {decoded_inputs}\")\n",
    "        \n",
    "        # Print the first row of labels, replacing ignore_index with the string '[IGNORE]'\n",
    "        labels = batch['labels'][1].tolist()\n",
    "        labels_str = ['[IGNORE]' if label == -100 else str(label) for label in labels]\n",
    "        print(f\"Labels: {labels_str}\")\n",
    "        print(len(labels))\n",
    "        \n",
    "        # Print the first row of the attention_mask\n",
    "        attention_mask_str = batch['attention_mask'][1].tolist()\n",
    "        print(f\"Attention mask: {attention_mask_str}\")\n",
    "    \n",
    "    # Optionally, display an image from the batch to visually confirm loading\n",
    "    if 'images' in batch:\n",
    "        image_tensor = batch['images'][1]\n",
    "        print(f\"First Row Image Data type: {image_tensor.dtype}\")\n",
    "        print(f\"First Row Image Shape: {image_tensor.shape}\")\n",
    "        print(f\"First Row Image Value range: [{image_tensor.min()}, {image_tensor.max()}]\")\n",
    "    \n",
    "    break  # Only check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "ignore_index = -100\n",
    "\n",
    "# subclass trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "#        outputs = model(**inputs)\n",
    "#        logits = outputs.logits\n",
    "#        print(\"Shape of logits: \", logits.shape)\n",
    "#        print(\"Shape of labels: \", labels.shape)\n",
    "#        loss = loss_eval(logits, labels)\n",
    "\n",
    "#        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            unwrapped_model = self.accelerator.unwrap_model(model)\n",
    "            if _is_peft_model(unwrapped_model):\n",
    "                model_name = unwrapped_model.base_model.model._get_name()\n",
    "            else:\n",
    "                model_name = unwrapped_model._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        #print(\"Shape of logits: \", logits.shape)\n",
    "        #print(\"Shape of labels: \", labels.shape)\n",
    "        negative_loss = -loss\n",
    "        return (negative_loss, outputs) if return_outputs else negative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "#cuda_idx = 2 # edit device index that you want to track\n",
    "#device = f'cuda:{cuda_idx}'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "output_model_name=f\"{model_name}-dogs-unlearned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    #max_steps=3, # Comment this out after training for TWO STEPS or THREE STEPS\n",
    "    output_dir=output_model_name,\n",
    "    learning_rate=1e-4,\n",
    "    # fp16=True, #for non ampere gpus\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1,\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim=\"adamw_torch\",\n",
    "    #gradient_checkpointing=True,\n",
    "    #gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post fine-tuning evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "adapter_path = \"liuhaotian/llava-v1.6-mistral-7b-dogs-unlearned/checkpoint-1\"\n",
    "model_path = \"liuhaotian/llava-v1.6-mistral-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    cache_dir='',\n",
    "    use_flash_attn=True,\n",
    "    # load_8bit=True #NOT SUPPORTED YET WITH THIS SCRIPT\n",
    "    # load_4bit=True #NOT SUPPORTED YET WITH THIS SCRIPT\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    adapter_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temporarily disable the transformation to access the original data\n",
    "eval_ds.reset_format()\n",
    "\n",
    "# Iterate over each example in the evaluation dataset\n",
    "for i in range(len(eval_ds)):\n",
    "    # Access the original image and caption for the current row\n",
    "    image = eval_ds[i]['image']\n",
    "    caption = eval_ds[i]['captions']\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "\n",
    "    eval_model(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        image_processor,\n",
    "        context_len,\n",
    "        image,\n",
    "        \"What do you see in this picture?\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
    "\n",
    "# Re-enable the transformation if needed\n",
    "eval_ds.set_transform(lambda batch: ds_transforms(batch, image_processor, tokenizer, model, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlearning with negative loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "#cuda_idx = 2 # edit device index that you want to track\n",
    "#device = f'cuda:{cuda_idx}'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "output_model_name=f\"{model_name}-dogs-unlearned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    #max_steps=3, # Comment this out after training for TWO STEPS or THREE STEPS\n",
    "    output_dir=output_model_name,\n",
    "    learning_rate=1e-4,\n",
    "    # fp16=True, #for non ampere gpus\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1,\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim=\"adamw_torch\",\n",
    "    #gradient_checkpointing=True,\n",
    "    #gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch 1 evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "adapter_path = \"liuhaotian/llava-v1.6-mistral-7b-dogs-unlearned/checkpoint-1\"\n",
    "model_path = \"liuhaotian/llava-v1.6-mistral-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    cache_dir='',\n",
    "    use_flash_attn=True,\n",
    "    # load_8bit=True #NOT SUPPORTED YET WITH THIS SCRIPT\n",
    "    # load_4bit=True #NOT SUPPORTED YET WITH THIS SCRIPT\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    adapter_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temporarily disable the transformation to access the original data\n",
    "eval_ds.reset_format()\n",
    "\n",
    "# Iterate over each example in the evaluation dataset\n",
    "for i in range(len(eval_ds)):\n",
    "    # Access the original image and caption for the current row\n",
    "    image = eval_ds[i]['image']\n",
    "    caption = eval_ds[i]['captions']\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "\n",
    "    eval_model(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        image_processor,\n",
    "        context_len,\n",
    "        image,\n",
    "        \"What do you see in this picture?\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
    "\n",
    "# Re-enable the transformation if needed\n",
    "eval_ds.set_transform(lambda batch: ds_transforms(batch, image_processor, tokenizer, model, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
