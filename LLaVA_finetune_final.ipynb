{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75afc84-9516-4042-ad37-977570e66341",
   "metadata": {},
   "source": [
    "### Prepare the dog images dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53482d88-1edf-4404-b810-d44dd8fd3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1131947-9d71-4d39-b1b0-726988148163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(filename):\n",
    "    \"\"\"Unzips a zip file.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the zip file to unzip.\n",
    "    \"\"\"\n",
    "\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "    print(f\"Unzipped {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c17528-ae05-46f6-bb14-46de03ece815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_coco(dataset_type, year='2017'):\n",
    "    \"\"\"Downloads a specific COCO dataset split.\n",
    "\n",
    "    Args:\n",
    "        dataset_type: 'train', 'val', or 'test'\n",
    "        year: The year of the dataset (default: '2017')\n",
    "    \"\"\"\n",
    "\n",
    "    base_url = 'http://images.cocodataset.org/zips/'\n",
    "    filename = f'{dataset_type}{year}.zip'\n",
    "    url = base_url + filename\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    # Download using requests with progress bar\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 #1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            f.write(data)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Download the datasets\n",
    "download_coco('train')\n",
    "download_coco('val')\n",
    "\n",
    "# Unzip the downloaded files\n",
    "unzip_file('train2017.zip')\n",
    "unzip_file('val2017.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0a36c-5ffe-4973-aed0-57425636a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('annotations/instances_train2017.json'):\n",
    "    print(\"instances_train2017.json file found!\")\n",
    "else:\n",
    "    print(\"instances_train2017.json file not found. Please check the download and extraction steps.\")\n",
    "\n",
    "if os.path.exists('annotations/captions_train2017.json'):\n",
    "    print(\"captions_train2017.json file found!\")\n",
    "else:\n",
    "    print(\"captions_train2017.json file not found. Please check the download and extraction steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964a2b2-f151-4160-a5ac-46a6cfdbfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load annotations \n",
    "with open('annotations/instances_train2017.json') as f:\n",
    "    train_annotations = json.load(f)\n",
    "with open('annotations/captions_train2017.json') as f:\n",
    "    caption_annotations = json.load(f)\n",
    "\n",
    "# 3. Create a mapping of image_id to captions\n",
    "image_id_to_captions = {}\n",
    "for ann in caption_annotations['annotations']:\n",
    "    image_id = ann['image_id']\n",
    "    caption = ann['caption']\n",
    "    if image_id not in image_id_to_captions:\n",
    "        image_id_to_captions[image_id] = []\n",
    "    image_id_to_captions[image_id].append(caption)\n",
    "\n",
    "# 4. Use pycocotools for convenient filtering\n",
    "coco = COCO('annotations/instances_train2017.json')\n",
    "catIds = coco.getCatIds(catNms=['dog'])\n",
    "imgIds = coco.getImgIds(catIds=catIds) \n",
    "images = coco.loadImgs(imgIds)\n",
    "\n",
    "# 5. Extract dog images\n",
    "image_dir = 'train2017'\n",
    "output_dir = 'dog_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for img in images:\n",
    "    try:\n",
    "        filename = img['file_name']\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        img = Image.open(img_path)\n",
    "        img.save(os.path.join(output_dir, filename))\n",
    "        print(f\"Saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95391c18-ee66-4672-8d8c-9a83a2e9f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# 6. Prepare data for Hugging Face Dataset\n",
    "data = []\n",
    "for img in images:\n",
    "    try:\n",
    "        filename = img['file_name']\n",
    "        img_id = img['id']\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "\n",
    "        # Get captions for this image\n",
    "        captions = image_id_to_captions.get(img_id, [])\n",
    "        if not captions:\n",
    "            print(f\"Warning: No captions found for {filename}\")\n",
    "            continue  # Skip images without captions\n",
    "\n",
    "        data.append({\n",
    "            'image': img_path,\n",
    "            'captions': captions \n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed71b64-d88a-4fd2-a96a-e32b426afd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "from datasets import Dataset, Image\n",
    "# Split data into train and test\n",
    "random.shuffle(data)  # Shuffle the data for randomness\n",
    "split_index = int(0.95 * len(data))\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "#print(\"Required batches: \", (len(data)/BATCH_SIZE))\n",
    "\n",
    "# 7. Create Hugging Face Datasets (batch by batch)\n",
    "def load_images_in_batches(data, batch_size):\n",
    "    all_images = []\n",
    "    all_captions = []  # Store all captions\n",
    "    batch_count = 0\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        for item in batch:\n",
    "            image = PIL.Image.open(item['image']).convert('RGB').resize((336, 336))\n",
    "            for caption in item['captions']:  # Iterate over captions\n",
    "                all_images.append(image.copy())  # Copy the image for each caption\n",
    "                all_captions.append(caption)\n",
    "\n",
    "    return all_images, all_captions  # Return both images and captions\n",
    "\n",
    "train_images, train_captions = load_images_in_batches(train_data, BATCH_SIZE)\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'image': train_images,\n",
    "    'captions': train_captions\n",
    "})\n",
    "print(\"Train dataset ready\")\n",
    "\n",
    "test_images, test_captions = load_images_in_batches(test_data, BATCH_SIZE)\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'image': test_images,\n",
    "    'captions': test_captions\n",
    "})\n",
    "print(\"Test dataset ready\")\n",
    "\n",
    "# Close all opened images after creating the datasets\n",
    "for img in train_dataset['image'] + test_dataset['image']:\n",
    "    img.close()\n",
    "\n",
    "print(\"Images processed and ready to push to hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab42b59-ac1b-4480-a26e-fa2c793a8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26a164-4ee8-4533-9185-b6df255ba4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a30d8-b28d-478e-a89c-9b8fe55c0cb1",
   "metadata": {},
   "source": [
    "## LLaVA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005986f-f9aa-45b6-98a5-e275e539e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"LLaVA\"):\n",
    "    !git clone https://github.com/haotian-liu/LLaVA\n",
    "else:\n",
    "    print(\"LLaVA directory already exists. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a408f9-3cba-4a10-a6cc-f567b0d5c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script replaces the vision_tower component\n",
    "# There could be some bugs with it. I recommend manually going to LLaVA/llava/model/builder.py, searching for all\n",
    "# instances of 'float' and replacing them with 'bfloat'\n",
    "import re\n",
    "\n",
    "# Define the path to the builder.py file\n",
    "file_path = 'LLaVA/llava/model/builder.py'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "# Regular expression to find the block between 'vision_tower = model.get_vision_tower()\n",
    "pattern_block = (\n",
    "    r'(vision_tower = model.get_vision_tower\\(\\)\\n)'\n",
    "    r'.*?' # Non-greedy match for any characters (including newlines)\n",
    "    r'(image_processor = vision_tower.image_processor)'\n",
    ")\n",
    "\n",
    "replacement_block = (\n",
    "    r'\\1' # keep the starting line unchanged\n",
    "    r'       if not vision_tower.is_loaded:\\n'\n",
    "    r'           print(\\'vision_tower is not loaded so loading it now\\')\\n'\n",
    "    r'           vision_tower.load_model(device_map=device_map)\\n'\n",
    "    r'           vision_tower.to(device=device, dtype=torch.bfloat16)\\n'\n",
    "    r'       else:\\n'\n",
    "    r'           print(\\'vision_tower is loaded\\)\\n'\n",
    "    r'      \\2' # Keep the ending line unchanged\n",
    ")\n",
    "\n",
    "# Replace the specific block\n",
    "content = re.sub(pattern_block, replacement_block, content, flags=re.DOTALL)\n",
    "\n",
    "# Write the modified content back to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(content)\n",
    "    \n",
    "print('The script has been updated successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da6fce-7e0e-4496-a282-c18e15f92bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script replaces all instances of float16 with bfloat16\n",
    "import re\n",
    "\n",
    "# Define the path to the builder.py file\n",
    "file_path = 'LLaVA/llava/model/builder.py'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Regular expression to find 'float16' not preceded by 'b'\n",
    "pattern = r'(?>!b)float16'\n",
    "\n",
    "# Check if there are any matches\n",
    "if re.search(pattern, content):\n",
    "    # Replace 'float16' with 'bfloat16'\n",
    "    modified_content = re.sub(pattern, 'bfloat16', content)\n",
    "    \n",
    "    # Write the modified content back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_content)\n",
    "    \n",
    "    print('All necessary instances of float16 have been replaced with bfloat16')\n",
    "else:\n",
    "    print('No replacement needed. All instances of float16 have already been replaced with bfloat16.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d35fcf-18b1-42d8-89b5-5cf4e1fcf5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9358c-240a-4f38-adcd-e92bd1b0ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead1fc3-f001-4368-8150-1f6b1e4b161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade Pillow -q\n",
    "!pip install -e \".[train]\" -q\n",
    "!pip install flash-attn --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc78a16-df4a-4154-a1ca-3c4fd62bb5a2",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a53e1-5de9-44fb-b3c4-d0e6fed2114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "import transformers\n",
    "from transformers import AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd20aa-f323-4bf8-94d3-79aff1a25324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867924c-d776-4730-a0da-57e9f3cc770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9b61f-d957-4664-b6c4-4dccb4718b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "import torch.nn as nn\n",
    "\n",
    "#cuda_idx = 2 # edit device index that you want to track\n",
    "#device = f'cuda:{cuda_idx}'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.6-mistral-7b\"\n",
    "#model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "#model_path = \"liuhaotian/llava-v1.6-vicuna-7b\"\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "print(model_name)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    cache_dir='',\n",
    "    use_flash_attn=True,\n",
    "    device=device,\n",
    "    #load_8bit = #NOT SUPPORTED\n",
    "    #load_4bit = #NOT SUPPORTED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e29b90-08ff-4592-9ed7-a2c7457bdb5e",
   "metadata": {},
   "source": [
    "## Examine the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9978c1d-4fc3-47af-93eb-c731df5b916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c59b78-26e3-4c94-8d9b-dc3bcf9abb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0776f-a667-448b-9308-93aa2dd61839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check dtype of all modules, focusing on those not torch.bfloat16\n",
    "print(\"Modules not torch.bfloat16:\")\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'parameters') and list(module.parameters()):\n",
    "        # Check if any parameter of the module is not bfloat16\n",
    "        if any(param.dtype != torch.bfloat16 for param in module.parameters()):\n",
    "            print(f\"{name}: {next(module.parameters()).dtype}\")\n",
    "        else:\n",
    "            # Optionally, acknowledge modules without parameters if needed\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547dc92-b3ef-4772-83ca-a661b883abfc",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b08b1-fbe6-4716-b090-671ec9dc6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed120f9c-952b-4d7c-894c-61264e731ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common function to create prompts\n",
    "def create_prompt (query, model, model_name=model_name, caption=None):\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in query:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            query = re.sub(IMAGE_PLACEHOLDER, image_token_se, query)\n",
    "        else:\n",
    "            query = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, query)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            query = image_token_se + \"\\n\" + query\n",
    "        else:\n",
    "            query = DEFAULT_IMAGE_TOKEN + \"\\n\" + query\n",
    "            \n",
    "    conv_mode = infer_conv_mode(model_name)\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], query)\n",
    "    if caption is not None:\n",
    "        conv.append_message(conv.roles[1], caption)\n",
    "    else:\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "# Common function to infer conversation mode\n",
    "def infer_conv_mode(model_name):\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        return \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        return \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        return \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        return \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        return \"mpt\"\n",
    "    else:\n",
    "        return \"llava_v0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17c6aa-605c-40aa-9e9e-4e7db746d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import PIL.Image\n",
    "\n",
    "def load_image(image_input):\n",
    "    # Check if the input is a string (path or URL)\n",
    "    if isinstance(image_input, str):\n",
    "        if image_input.startswith(\"http\") or image_input.startswith(\"https\"):\n",
    "            response = requests.get(image_input)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(image_input).convert(\"RGB\")\n",
    "    elif isinstance(image_input, PIL.Image.Image):\n",
    "        # Input is already an Image object, return as is\n",
    "        image = image_input\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image input type\")\n",
    "    return image\n",
    "\n",
    "# Common function to process images\n",
    "def process_and_prepare_images(image_files, image_processor, model, device):\n",
    "    images = [load_image(image_file) for image_file in image_files]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(\n",
    "        device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    image_sizes = [image.size for image in images]\n",
    "    return images_tensor, image_sizes\n",
    "\n",
    "def eval_model(tokenizer, model, image_processor, context_len, image_file, query, model_name=model_name, sep=\",\", temperature=1.0, num_beams=1, max_new_tokens=512):\n",
    "    # Model\n",
    "    disable_torch_init()\n",
    "    \n",
    "    # Create prompt using the common function\n",
    "    prompt = create_prompt(query, model, model_name)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Process images using the common function\n",
    "    if isinstance(image_file, list):\n",
    "        images_tensor, image_sizes = process_and_prepare_images(image_file, image_processor, model, model.device)\n",
    "    elif isinstance(image_file, str):\n",
    "        images_tensor, image_sizes = process_and_prepare_images([image_file], image_processor, model, model.device)\n",
    "    else:\n",
    "        # If image_file is neither a list nor a string, it's likely an Image object or similar; wrap it in a list\n",
    "        images = [image_file]\n",
    "        images_tensor, image_sizes = process_and_prepare_images(images, image_processor, model, model.device)\n",
    "\n",
    "    images_tensor.to(model.device)\n",
    "    image_sizes.to(device) if isinstance(image_sizes, torch.Tensor) else image_sizes\n",
    "    \n",
    "    # Tokenixe the prompt using the custom tokenizer_image_token function\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .to(model.device)\n",
    "    )\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=temperature != 1.0,\n",
    "            temperature=temperature,\n",
    "            # top_p=top_p,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=False)[0].strip()\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51f70b-94ef-4f8a-a4e3-746805357d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Dog image URL\n",
    "image_url = 'https://farm7.staticflickr.com/6119/6315804553_050a2d1f4e_z.jpg'\n",
    "\n",
    "# Download the image and open it with PIL\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Display the image using matplotplib\n",
    "plt.imshow(image)\n",
    "plt.axis('off') # Turn off axis numbers and ticks\n",
    "plt.show()\n",
    "\n",
    "# Pass the processed image to eval_model\n",
    "eval_model(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    image_processor,\n",
    "    context_len,\n",
    "    image, # using the processed image\n",
    "    \"What do you see in this picure?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c823699-45ba-4a9c-bdaa-de0d8c5573e6",
   "metadata": {},
   "source": [
    "## Prepare the dog images dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80985992-f25c-4542-9c71-857b136ff568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_create_labels(example_batch, image_processor, tokenizer, model, device, model_name=model_name, ignore_index=-100, image_token_index=IMAGE_TOKEN_INDEX):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = example_batch['image']\n",
    "    \n",
    "    images_tensor, image_sizes = process_and_prepare_images(image_files, image_processor, model, device)\n",
    "\n",
    "    query = \"What do you see in this picture?\"\n",
    "    \n",
    "    # Tokenize the conversation without the captions to determine which tokens to ignore\n",
    "    tokenized_conversations_without_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, None), tokenizer, image_token_index, return_tensors=\"pt\")\n",
    "        for _ in example_batch['captions']\n",
    "    ]\n",
    "    \n",
    "    # Tokenize the full conversations with the captions\n",
    "    tokenized_conversations_with_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, caption), tokenizer, image_token_index, return_tensors=\"pt\")\n",
    "        for caption in example_batch['captions']\n",
    "    ]\n",
    "    \n",
    "    # Pad the tokenized conversations to the same length\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversations_with_caption], batch_first=True, padding_value=pad_token_id).to(device)\n",
    "    \n",
    "    # Create attention_mask (1 for real tokens and 0 for padding tokens)\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "    \n",
    "    # Create the labels tensor which is a copy of input_ids but with ignore_index for non-caption tokens\n",
    "    labels = torch.full_like(input_ids, fill_value=ignore_index)\n",
    "    for i, tcwc in enumerate(tokenized_conversations_without_caption):\n",
    "        # Set ignore_index for the tokens corresponding to the conversation without the caption\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
    "\n",
    "    print(\"Labels shape: \", labels.shape)\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"images\": images_tensor,\n",
    "        \"image_sizes\": image_sizes,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "    \n",
    "# Make sure to define the function outside of the lambda to ensure it's serializable\n",
    "def transform_batch(batch):\n",
    "    return tokenize_and_create_labels(batch, image_processor, tokenizer, model, device, model_name)\n",
    "\n",
    "train_ds = train_dataset #ds[\"train\"]\n",
    "eval_ds = test_dataset #ds[\"test\"]\n",
    "\n",
    "# Apply the transformation function to the dataset\n",
    "train_ds.set_transform(transform_batch)\n",
    "eval_ds.set_transform(transform_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c07b30-e6d6-426e-88e2-34d772262e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d70106-5f9b-4552-bd79-dbfbfa3e79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4ab1b-1614-4b29-885b-98696ba91c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc46c31-60b0-43cf-bfba-b88728fe5b7c",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbad00-16c2-4374-858b-23a005bfa045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        # \"fc1\", \"fc2\", # for llama,\n",
    "        \"mm_projector\" #for mistral, train instead \"mm_projector\"\n",
    "        \"up_proj\", \"down_proj\", \"gate_proj\" #optionally train more linarly\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0e466-a61f-4778-a7e9-f5da6da6989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884be31c-c13a-458d-9234-6a16f84354fe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9834a0-3e1e-430f-82c3-f21bbcb3ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_ds is your training dataset prepared as a PyTorch Dataset object\n",
    "batch_size = 4  # Specify the batch size you want to use\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Assuming train_loader is your DataLoader instance for the training dataset\n",
    "for batch in train_loader:\n",
    "    batch_size = len(batch)\n",
    "    print(batch.keys())  # Print the dictionary keys to see what data is included in a batch\n",
    "    \n",
    "    # If 'images' is a key, this indicates that images are being loaded\n",
    "    if 'images' in batch:\n",
    "        print(\"Images are included in the DataLoader.\")\n",
    "        print(f\"Batch 'images' shape: {batch['images'].shape}\")  # Print the shape of the images tensor\n",
    "        \n",
    "    # Similarly, check for other expected keys, like 'input_ids' and 'attention_mask'\n",
    "    if 'input_ids' in batch and 'attention_mask' in batch:\n",
    "        # Print the first row of input_ids to check for out-of-range token IDs\n",
    "        input_ids_first_row = batch['input_ids'][1]\n",
    "        print(f\"First row of 'input_ids': \\n{input_ids_first_row.tolist()}\")\n",
    "            \n",
    "        print(\"Text inputs are included in the DataLoader.\")\n",
    "        print(f\"Batch 'input_ids' shape: {batch['input_ids'].shape}\")\n",
    "        print(f\"Batch 'attention_mask' shape: {batch['attention_mask'].shape}\")\n",
    "        \n",
    "        # Print the first row of labels, replacing ignore_index with the string '[IGNORE]'\n",
    "        labels = batch['labels'][1].tolist()\n",
    "        labels_str = ['[IGNORE]' if label == -100 else str(label) for label in labels]\n",
    "        print(f\"Labels: {labels_str}\")\n",
    "        print(len(labels))\n",
    "        \n",
    "        # Print the first row of the attention_mask\n",
    "        attention_mask_str = batch['attention_mask'][1].tolist()\n",
    "        print(f\"Attention mask: {attention_mask_str}\")\n",
    "    \n",
    "    # Optionally, display an image from the batch to visually confirm loading\n",
    "    if 'images' in batch:\n",
    "        image_tensor = batch['images'][1]\n",
    "        print(f\"First Row Image Data type: {image_tensor.dtype}\")\n",
    "        print(f\"First Row Image Shape: {image_tensor.shape}\")\n",
    "        print(f\"First Row Image Value range: [{image_tensor.min()}, {image_tensor.max()}]\")\n",
    "    \n",
    "    break  # Only check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2549d-535a-41ac-9af3-6db3a89fd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "ignore_index = -100\n",
    "\n",
    "# subclass trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "#        outputs = model(**inputs)\n",
    "#        logits = outputs.logits\n",
    "#        print(\"Shape of logits: \", logits.shape)\n",
    "#        print(\"Shape of labels: \", labels.shape)\n",
    "#        loss = loss_eval(logits, labels)\n",
    "\n",
    "#        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            unwrapped_model = self.accelerator.unwrap_model(model)\n",
    "            if _is_peft_model(unwrapped_model):\n",
    "                model_name = unwrapped_model.base_model.model._get_name()\n",
    "            else:\n",
    "                model_name = unwrapped_model._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        #print(\"Shape of logits: \", logits.shape)\n",
    "        #print(\"Shape of labels: \", labels.shape)\n",
    "        negative_loss = -loss\n",
    "        return (negative_loss, outputs) if return_outputs else negative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd082081-dfcd-4a0f-b963-217781f63634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "#cuda_idx = 2 # edit device index that you want to track\n",
    "#device = f'cuda:{cuda_idx}'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "output_model_name=f\"{model_name}-dogs-unlearned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=3, # Comment this out after training for TWO STEPS or THREE STEPS\n",
    "    output_dir=output_model_name,\n",
    "    learning_rate=1e-4,\n",
    "    # fp16=True, #for non ampere gpus\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1,\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim=\"adamw_torch\",\n",
    "    #gradient_checkpointing=True,\n",
    "    #gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5190f3f-d46f-436a-adc3-09d9da89a57a",
   "metadata": {},
   "source": [
    "## Post-training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107b558-b549-4762-8825-a40f6328f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "adapter_path = \"liuhaotian/llava-v1.6-mistral-7b-dogs-unlearned/checkpoint-1\"\n",
    "model_path = \"liuhaotian/llava-v1.6-mistral-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    cache_dir='',\n",
    "    use_flash_attn=True,\n",
    "    # load_8bit=True #NOT SUPPORTED YET WITH THIS SCRIPT\n",
    "    # load_4bit=True #NOT SUPPORTED YET WITH THIS SCRIPT\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    adapter_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330add38-4b08-4bd6-be51-c07b6952d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temporarily disable the transformation to access the original data\n",
    "eval_ds.reset_format()\n",
    "\n",
    "# Iterate over each example in the evaluation dataset\n",
    "for i in range(len(eval_ds)): # change the length to run the evaluation for a select few images\n",
    "    # Access the original image and caption for the current row\n",
    "    image = eval_ds[i]['image']\n",
    "    caption = eval_ds[i]['captions']\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "\n",
    "    eval_model(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        image_processor,\n",
    "        context_len,\n",
    "        image,\n",
    "        \"What do you see in this picture?\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
    "\n",
    "# Re-enable the transformation if needed\n",
    "eval_ds.set_transform(lambda batch: ds_transforms(batch, image_processor, tokenizer, model, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
