{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f64357-19f0-4f83-9ece-88e0f7e36fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable fast weights download and upload\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29553b04-abc4-440b-a2c9-90b7c63eec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment Setup and Imports\n",
    "import torch\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import re\n",
    "from typing import Dict, Any, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from transformers import (\n",
    "    MllamaForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a701e17a-2f02-4448-85e6-c1e8affd62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Configuration and Setup\n",
    "class LLMManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "        use_4bit: bool = False\n",
    "    ):\n",
    "        self.model_id = model_id\n",
    "        self.use_4bit = use_4bit\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the model with optional 4-bit quantization\"\"\"\n",
    "        if self.use_4bit:\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            self.model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                self.model_id,\n",
    "                quantization_config=quant_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            self.model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                self.model_id,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        \n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_id)\n",
    "    \n",
    "    def reason_about_guardrails(\n",
    "        self,\n",
    "        plugin_outputs: Dict[str, Any],\n",
    "        guard_rules: Dict[str, Any],\n",
    "        original_input: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Stub implementation of guardrail reasoning\n",
    "        In production: Would craft prompt and use LLM inference\n",
    "        \"\"\"\n",
    "        # Simple logic for demonstration\n",
    "        has_violations = any(\n",
    "            output.get(\"violation\", False)\n",
    "            for output in plugin_outputs.values()\n",
    "        )\n",
    "        \n",
    "        violations = [\n",
    "            f\"{plugin}: {output['reason']}\"\n",
    "            for plugin, output in plugin_outputs.items()\n",
    "            if output.get(\"violation\", False)\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"verdict\": \"deny\" if has_violations else \"allow\",\n",
    "            \"reason\": \"; \".join(violations) if violations else \"No violations detected\",\n",
    "            \"plugin_outputs\": plugin_outputs\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f4eda2-37c0-4369-9faa-2357d8361f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Abstract interface for NSFW detection to support multiple backends\n",
    "\"\"\"\n",
    "\n",
    "class NSFWDetectorInterface:\n",
    "    def load_model(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def preprocess_image(self, image_input: Any) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def detect(self, preprocessed_image: torch.Tensor) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Cell 3: CLIP-based NSFW Detector [NEW]\n",
    "\"\"\"\n",
    "CLIP-based implementation of NSFW detection\n",
    "\"\"\"\n",
    "\n",
    "class CLIPNSFWDetector(NSFWDetectorInterface):\n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.nsfw_concepts = [\n",
    "            \"explicit content\",\n",
    "            \"nude\",\n",
    "            \"pornographic\",\n",
    "            \"safe for work\",\n",
    "            \"appropriate content\",\n",
    "            \"family friendly\"\n",
    "        ]\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load CLIP model and processor\"\"\"\n",
    "        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(self.model_name)\n",
    "        return self\n",
    "    \n",
    "    def preprocess_image(self, image_input: Any) -> torch.Tensor:\n",
    "        \"\"\"Preprocess image for CLIP\"\"\"\n",
    "        if isinstance(image_input, str):\n",
    "            # In practice, you'd load the image here\n",
    "            # For demo, we'll create a dummy image\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "        elif isinstance(image_input, Image.Image):\n",
    "            image = image_input\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image input type\")\n",
    "        \n",
    "        return self.processor(\n",
    "            images=image,\n",
    "            text=self.nsfw_concepts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def detect(self, preprocessed_input: Dict[str, torch.Tensor]) -> float:\n",
    "        \"\"\"\n",
    "        Detect NSFW content using CLIP\n",
    "        Returns: float between 0 and 1 (higher = more likely NSFW)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**preprocessed_input)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            probs = logits_per_image.softmax(dim=-1)\n",
    "            \n",
    "            # Average the probabilities of NSFW concepts (first 3)\n",
    "            nsfw_prob = probs[0, :3].mean().item()\n",
    "            # Average the probabilities of SFW concepts (last 3)\n",
    "            sfw_prob = probs[0, 3:].mean().item()\n",
    "            \n",
    "            # Normalize to get NSFW score\n",
    "            nsfw_score = nsfw_prob / (nsfw_prob + sfw_prob)\n",
    "            \n",
    "            return nsfw_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b54eef1a-f895-44d9-9665-96b8ed08a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plugin Base and Implementations\n",
    "class BasePlugin:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "    \n",
    "    def should_trigger(self, normalized_inputs: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Determine if plugin should run for given input\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def check(self, normalized_inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Run the plugin's checks\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TextPrivacyPlugin(BasePlugin):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"text_privacy\")\n",
    "        self.ssn_pattern = re.compile(r'\\d{3}-\\d{2}-\\d{4}')\n",
    "    \n",
    "    def should_trigger(self, normalized_inputs: Dict[str, Any]) -> bool:\n",
    "        return bool(normalized_inputs.get(\"text\"))\n",
    "    \n",
    "    def check(self, normalized_inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        text = normalized_inputs.get(\"text\", \"\")\n",
    "        if self.ssn_pattern.search(text):\n",
    "            return {\n",
    "                \"violation\": True,\n",
    "                \"reason\": \"Detected SSN pattern in text\"\n",
    "            }\n",
    "        return {\n",
    "            \"violation\": False,\n",
    "            \"reason\": \"No privacy violations detected\"\n",
    "        }\n",
    "\n",
    "class NSFWImagePlugin(BasePlugin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        threshold: float = 0.5,\n",
    "        detector: Optional[NSFWDetectorInterface] = None\n",
    "    ):\n",
    "        super().__init__(\"nsfw_image\")\n",
    "        self.threshold = threshold\n",
    "        self.detector = detector or CLIPNSFWDetector()\n",
    "        self.detector.load_model()\n",
    "    \n",
    "    def should_trigger(self, normalized_inputs: Dict[str, Any]) -> bool:\n",
    "        return bool(normalized_inputs.get(\"image\"))\n",
    "    \n",
    "    def check(self, normalized_inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Check if image contains NSFW content using CLIP\"\"\"\n",
    "        image = normalized_inputs.get(\"image\")\n",
    "        \n",
    "        try:\n",
    "            # Preprocess image\n",
    "            inputs = self.detector.preprocess_image(image)\n",
    "            \n",
    "            # Get NSFW score\n",
    "            nsfw_score = self.detector.detect(inputs)\n",
    "            \n",
    "            if nsfw_score > self.threshold:\n",
    "                return {\n",
    "                    \"violation\": True,\n",
    "                    \"reason\": f\"NSFW content detected (score: {nsfw_score:.2f})\",\n",
    "                    \"score\": nsfw_score\n",
    "                }\n",
    "            return {\n",
    "                \"violation\": False,\n",
    "                \"reason\": f\"Content appears safe (score: {nsfw_score:.2f})\",\n",
    "                \"score\": nsfw_score\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"violation\": True,\n",
    "                \"reason\": f\"Error processing image: {str(e)}\",\n",
    "                \"score\": None\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9489d24-b6f8-4ccd-906f-927fa8d98dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#M3Guard Orchestrator\n",
    "\"\"\"\n",
    "Implements the main orchestrator that coordinates plugins and LLM reasoning\n",
    "\"\"\"\n",
    "\n",
    "class M3GuardOrchestrator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_manager: LLMManager,\n",
    "        guard_rules: Dict[str, Any]\n",
    "    ):\n",
    "        self.llm_manager = llm_manager\n",
    "        self.guard_rules = guard_rules\n",
    "        self.registry_of_plugins: List[BasePlugin] = []\n",
    "    \n",
    "    def register_plugin(self, plugin: BasePlugin):\n",
    "        self.registry_of_plugins.append(plugin)\n",
    "    \n",
    "    def universal_preprocessing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Normalize inputs for plugin processing\"\"\"\n",
    "        return {\n",
    "            \"text\": input_data.get(\"text\", \"\"),\n",
    "            \"image\": input_data.get(\"image\"),\n",
    "            \"metadata\": input_data.get(\"metadata\", {})\n",
    "        }\n",
    "    \n",
    "    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Main processing pipeline\"\"\"\n",
    "        normalized_inputs = self.universal_preprocessing(input_data)\n",
    "        plugin_outputs = {}\n",
    "        \n",
    "        for plugin in self.registry_of_plugins:\n",
    "            if plugin.should_trigger(normalized_inputs):\n",
    "                plugin_result = plugin.check(normalized_inputs)\n",
    "                plugin_outputs[plugin.name] = plugin_result\n",
    "        \n",
    "        final_decision = self.llm_manager.reason_about_guardrails(\n",
    "            plugin_outputs,\n",
    "            self.guard_rules,\n",
    "            input_data\n",
    "        )\n",
    "        \n",
    "        return final_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60788248-2660-4a63-8b09-06d0a8a92960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guard Rules and Test Data\n",
    "\"\"\"\n",
    "Define guard rules and create test cases\n",
    "\"\"\"\n",
    "\n",
    "# Guard rules\n",
    "guard_rules = {\n",
    "    \"deny_if_ssn\": True,\n",
    "    \"nsfw_threshold\": 0.5,\n",
    "    \"require_all_plugins\": False\n",
    "}\n",
    "\n",
    "# Test data\n",
    "test_inputs = [\n",
    "    {\n",
    "        \"text\": \"My SSN is 123-45-6789\",\n",
    "        \"image\": None,\n",
    "        \"metadata\": {\"source\": \"text_only\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Hello, this is a safe message\",\n",
    "        \"image\": None,\n",
    "        \"metadata\": {\"source\": \"text_only\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Check out this image\",\n",
    "        \"image\": \"Arka/Agentic Guardrails/trump Luther King.jpeg\",  # In practice: PIL.Image object\n",
    "        \"metadata\": {\"source\": \"mixed\"}\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e869f87a-51ff-4ab6-83b7-299b4d370bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd08030a0a9e450b8a3e73cb9a1999aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6e39acb03c400682ac93c146662c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc3280582094858a7117a1aebee497a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0199e432fee8472dba0daa4d4f316cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449c61e483224dcfaaa5f93866a8a256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401595e59d7b4d2ea32bad4361841911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfb4ef1455d4f549fc6b3bfabf5990a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aec1f4a69134fdeb5454e02abb61125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4620c5fcede44128baa4ca9558366d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== M3Guard Test Results ===\n",
      "\n",
      "Test Case 1:\n",
      "Input: {'text': 'My SSN is 123-45-6789', 'image': None, 'metadata': {'source': 'text_only'}}\n",
      "Result: {'verdict': 'deny', 'reason': 'text_privacy: Detected SSN pattern in text', 'plugin_outputs': {'text_privacy': {'violation': True, 'reason': 'Detected SSN pattern in text'}}}\n",
      "\n",
      "Test Case 2:\n",
      "Input: {'text': 'Hello, this is a safe message', 'image': None, 'metadata': {'source': 'text_only'}}\n",
      "Result: {'verdict': 'allow', 'reason': 'No violations detected', 'plugin_outputs': {'text_privacy': {'violation': False, 'reason': 'No privacy violations detected'}}}\n",
      "\n",
      "Test Case 3:\n",
      "Input: {'text': 'Check out this image', 'image': 'Arka/Agentic Guardrails/trump Luther King.jpeg', 'metadata': {'source': 'mixed'}}\n",
      "Result: {'verdict': 'allow', 'reason': 'No violations detected', 'plugin_outputs': {'text_privacy': {'violation': False, 'reason': 'No privacy violations detected'}, 'nsfw_image': {'violation': False, 'reason': 'Content appears safe (score: 0.37)', 'score': 0.3742016405818891}}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM Manager\n",
    "llm_manager = LLMManager(use_4bit=False)  # Toggle 4-bit as needed\n",
    "\n",
    "# Initialize Orchestrator\n",
    "orchestrator = M3GuardOrchestrator(llm_manager, guard_rules)\n",
    "\n",
    "# Register plugins\n",
    "orchestrator.register_plugin(TextPrivacyPlugin())\n",
    "orchestrator.register_plugin(NSFWImagePlugin(threshold=0.5))\n",
    "\n",
    "# Process test inputs\n",
    "print(\"=== M3Guard Test Results ===\")\n",
    "for i, test_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\nTest Case {i}:\")\n",
    "    print(f\"Input: {test_input}\")\n",
    "    result = orchestrator.process_input(test_input)\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d9798e3-5d15-4910-9596-10017c45cf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4534241413b2470f84619a418701c2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== M3Guard Test Results ===\n",
      "\n",
      "Test Case 1:\n",
      "Input: {'text': 'Check out this image', 'image': 'Arka/Agentic Guardrails/plant.jpg', 'metadata': {'source': 'mixed'}}\n",
      "Result: {'verdict': 'allow', 'reason': 'No violations detected', 'plugin_outputs': {'text_privacy': {'violation': False, 'reason': 'No privacy violations detected'}, 'nsfw_image': {'violation': False, 'reason': 'Content appears safe (score: 0.37)', 'score': 0.3742016405818891}}}\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "test_inputs = [\n",
    "    {\n",
    "        \"text\": \"Check out this image\",\n",
    "        \"image\": \"Arka/Agentic Guardrails/plant.jpg\",  # In practice: PIL.Image object\n",
    "        \"metadata\": {\"source\": \"mixed\"}\n",
    "    }\n",
    "]\n",
    "# Initialize LLM Manager\n",
    "llm_manager = LLMManager(use_4bit=False)  # Toggle 4-bit as needed\n",
    "\n",
    "# Initialize Orchestrator\n",
    "orchestrator = M3GuardOrchestrator(llm_manager, guard_rules)\n",
    "\n",
    "# Register plugins\n",
    "orchestrator.register_plugin(TextPrivacyPlugin())\n",
    "orchestrator.register_plugin(NSFWImagePlugin(threshold=0.5))\n",
    "\n",
    "# Process test inputs\n",
    "print(\"=== M3Guard Test Results ===\")\n",
    "for i, test_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\nTest Case {i}:\")\n",
    "    print(f\"Input: {test_input}\")\n",
    "    result = orchestrator.process_input(test_input)\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f83f66-e1b1-40a8-8d65-f89130530df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
